# Imitation Learning
In this assignment, we implemented two different forms of imitation learning, specifically Behavior Cloning and DAgger (Dataset Aggregation). Imitation Learning approaches a given environment space as a Markov Decision Process (MDP), with a given state space, action space, P(s'|s, a) transition distribution to move between different states given a particular state-action pair, and an unknown R(s,a) reward function. Training is done with access to a given "expert" optimal policy π*, and the learned policy π trains from trajectories generated by this expert policy. Both of these algorithms were trained within the "Cartpole-v0" environment of OpenAI's Gym toolkit (https://gym.openai.com/envs/CartPole-v0/). Written in Python, using Numpy & Keras/Tensorlow. 


## Behaviour Cloning

Behaviour Cloning is a supervised form of imitation learning, where the emphasis is on learning the expert's policy from its trajectory data. This algorithm can be broken down in into three steps.

1. Collect expert trajectories from the expert policy
2. Treat the trajectories as independently and identically distributed state-action pairs (s0, a0), (s1, a1) ...
3. Learn the policy π by minimizing the divergence L(a0, π(s)) between the expert policy and the distribution outputted by the policy. 

## DAgger
In this algorithm, as with Behaviour Cloning we consult with the expert policy. However, in each episode, or rollout, we generated an episode with our untrained policy, collected the states visited during each trajectory, and generated state-action pairs to aggregate to the dataset, by consulting with our expert policy during each episode. This policy was trained according to this algorithm for 100 total iterations, with varying number of episodes rolled out per iteration.

Reference Links: https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c

https://cmudeeprl.github.io/Spring202010403website/assets/lectures/s20_lecture5.pdf


